# Milgram Experiment Simulation with LLMs

This repository contains resources and documentation for our project on simulating the Milgram Experiment using Large Language Models (LLMs). Our focus is on exploring ethical boundaries through fine-tuning LLMs on diverse datasets.

## Project Structure

- `Chatting_notebook-final.ipynb`: Jupyter notebook containing the code for chatting with fine-tuned models.
- `Sample_Chats.ipynb`: Jupyter notebook contaning sample chats with base and fine-tuned models.
- `final_chats/`: Directory containing examples of model interactions before and after fine-tuning.
- `Dataset_Preprocessing.ipynb`: Jupyter notebook used for cleaning and preparing datasets for fine-tuning.
- `Finetuning_script.py`: Script for quantizing and fine-tuning the models. 

## Usage

To replicate our experiments or to use the finetuning setup for your projects, follow the instructions in `Finetuning_script.py` after installing `requirements.txt`.

## Fine-Tuned Models

Our fine-tuned models (Mistralv0.1 and Llama-2) can be found on this Google Drive [Link](https://drive.google.com/file/d/1BZGm9BP7xxu3VRML3itoDL3ZFcE7hbBf/view?usp=sharing)
